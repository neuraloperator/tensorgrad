default: &DEFAULT

  #General
  # For computing compression
  n_params_baseline: None
  verbose: True
  arch: 'fno1d'

  #Distributed computing
  distributed:
    use_distributed: False
    wireup_info: 'mpi'
    wireup_store: 'tcp'
    model_parallel_size: 2
    seed: 666

  
  # FNO related
  fno1d:
    in_channels: 2
    out_channels: 1
    modes: 90
    hidden_channels: 128
    lifting_channels: 256
    projection_channels: 256
    n_layers: 4
    domain_padding: None
    domain_padding_mode: 'one-sided'
    fft_norm: 'forward'
    norm: 'group_norm'
    skip: 'linear'
    implementation: 'factorized'
    separable: 0
    preactivation: 0
    half_prec_fourier: False
    half_prec_inverse: False
    stabilizer: None

    use_mlp: 1
    mlp:
        expansion: 0.5
        dropout: 0

    factorization: None
    rank: 0.05
    fixed_rank_modes: None
    dropout: 0.0
    tensor_lasso_penalty: 0.0
    joint_factorization: False

  # Optimizer
  opt:
    n_epochs: 10000
    learning_rate: 0.0001
    training_loss: 'l2'
    weight_decay: 1e-4
    amp_autocast: False

    scheduler_T_max: 500
    scheduler_patience: 100
    scheduler: 'ReduceLROnPlateau'
    step_size: 60
    gamma: 0.5
    per_layer_opt: False
    tensorgrad: True
    svd_type: 'truncated_svd'  # Options: 'randomized_svd' or 'truncated_svd'
    tensorgrad_scale: 1.0
    activation_checkpoint: False
    naive_galore: False
    naive_galore_support_complex: True
    first_dim_rollup: 1
    update_proj_gap: 50
    galore_proj_type: std

    # New parameters from NS config
    checkpointing: False
    profiling: False
    max_batches: None
    resume_from_dir: "./ckpts"
    save_dir: "./ckpts"
    save_every: 10
    tucker_warm_restart: True
    adamw_support_complex: True
    update_proj_gap_end: 100
    update_proj_gap_mode: 'fixed'
    n_iter_max_tucker: 1
    optimizer_type: 'adamw'
    galore_2d_proj_type: left
    projector_use_mixed_precision: False
    
    proj_type: "low_rank"
    rank: [1.0]
    sparse_ratio: [0.5]
    sparse_type: "randk"
    scale: 1.0
    scale_by_mask_ratio: False
    second_proj_type: "unstructured_sparse"
    second_sparse_ratio: [0.05]
    second_sparse_type: "randk"
    second_scale: 1.0
    second_rank: [1.0]
    second_scale_by_mask_ratio: False
    log_scaling_factors: False
    memory_efficient_optim: None
    low_rank_cautious: False
    reset_optimizer_states: False

  # Dataset related
  data:
    folder: 'data/burgers' 
    batch_size: 16
    n_train: 800
    test_batch_sizes: [16]
    n_tests: [200]
    spatial_length: 8192
    temporal_length: 1

    positional_encoding: True
    encode_input: False
    encode_output: False
    include_endpoint: [True, False]

  profile: True

  # Patching
  patching:
    levels: 0
    padding: 0
    stitching: False

  # Weights and biases
  wandb:
    log: True
    name: "" # If None, config will be used but you can override it here
    group: None
    project: "tensorgrad" 
    entity: "wandb_entity" # put your username here
    sweep: False
    log_output: True
    log_test_interval: 1
    resume: "allow"
    log_train_interval: -1 # -1 means no logging and 1 means every step, N means every N steps
    log_ranks_interval: -1 # -1 means no logging and 1 means every step, N means every N steps
